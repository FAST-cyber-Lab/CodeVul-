# -*- coding: utf-8 -*-
"""predict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QNMr3XYwdp7-z3jfZx-ZS4raGj7Fba7o
"""

import os
import argparse
import numpy as np
import pandas as pd
import pickle
import zipfile
import gdown
from sklearn.preprocessing import StandardScaler

def download_and_extract_models(file_id, output_dir="saved_models", zip_name="saved_models.zip"):

    os.makedirs(output_dir, exist_ok=True)
    zip_path = os.path.join(output_dir, zip_name)

    if not os.path.exists(zip_path):
        print("Downloading model zip from Google Drive...")
        url = f"https://drive.google.com/uc?id={file_id}"
        gdown.download(url, zip_path, quiet=False)
    else:
        print("Model zip already exists")

    print("Extracting zip file...")
    if zipfile.is_zipfile(zip_path):
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(output_dir)
        print(f"Models extracted to: {output_dir}")
    else:
        print(f"The downloaded file '{zip_path}' is not a valid zip file")

def load_models(models_dir="saved_models/saved_models"):

    models = {}
    model_paths = {
        'svm': os.path.join(models_dir, 'svm_model.pkl'),
        'xgb': os.path.join(models_dir, 'xgb_model.pkl'),
        'extra_tree': os.path.join(models_dir, 'extra_tree_model.pkl'),
        'log_reg': os.path.join(models_dir, 'log_reg_model.pkl'),
        'nn': os.path.join(models_dir, 'nn_model.pkl'),
        'scaler': os.path.join(models_dir, 'scaler.pkl')
    }

    for model_name, model_path in model_paths.items():
        if os.path.exists(model_path):
            with open(model_path, 'rb') as f:
                models[model_name] = pickle.load(f)
            print(f"Loaded {model_name} model")
        else:
            print(f"Warning: Could not find {model_name} model at {model_path}")

    return models

def stacked_prediction(X_new, models):

    svm_probs = models['svm'].predict_proba(X_new)
    xgb_probs = models['xgb'].predict_proba(X_new)
    extra_tree_probs = models['extra_tree'].predict_proba(X_new)
    log_reg_probs = models['log_reg'].predict_proba(X_new)

    stacked_probs = np.hstack((svm_probs, xgb_probs, extra_tree_probs, log_reg_probs))
    predictions = models['nn'].predict(stacked_probs)
    probabilities = models['nn'].predict_proba(stacked_probs)

    return predictions, probabilities

def main():
    parser = argparse.ArgumentParser(description="Predict code vulnerabilities using CodeVul+ embeddings")
    parser.add_argument("--embeddings", type=str, required=True, help="Path to the embeddings CSV file generated by infer.py")
    parser.add_argument("--dataset", type=str, required=False, help="Optional path to original dataset to merge results")
    parser.add_argument("--output", type=str, default="vulnerability_predictions.csv", help="Path to save prediction results")
    parser.add_argument("--model_dir", type=str, default="saved_models/saved_models", help="Directory containing saved models")
    parser.add_argument("--file_id", type=str, default="1dQZOOY80DxYCtUZz962yQn-5JyuZvtNP", help="Google Drive file ID for models")
    parser.add_argument("--download", action="store_true", help="Force download models")

    args = parser.parse_args()


    if args.download or not os.path.exists(args.model_dir):
        download_and_extract_models(args.file_id)

    models = load_models(args.model_dir)

    print(f"Loading embeddings from {args.embeddings}")
    embeddings_df = pd.read_csv(args.embeddings)

    X = embeddings_df.values
    if 'scaler' in models:
        print("Scaling features...")
        X = models['scaler'].transform(X)
    else:
        print("Warning: No scaler found, using raw embeddings")

    print("Making predictions...")
    predictions, probabilities = stacked_prediction(X, models)


    results_df = pd.DataFrame({
        'prediction': predictions,
        'vulnerability_probability': probabilities[:, 1]
    })


    if args.dataset and os.path.exists(args.dataset):
        print(f"Merging predictions with original dataset from {args.dataset}")
        original_df = pd.read_csv(args.dataset)

        if len(original_df) == len(results_df):
            results_df = pd.concat([original_df, results_df], axis=1)
        else:
            print(f"Warning: Original dataset has {len(original_df)} rows but predictions have {len(results_df)} rows")

    results_df.to_csv(args.output, index=False)
    print(f"Predictions saved to {args.output}")

    vulnerable_count = sum(predictions == 1)
    total_count = len(predictions)
    print(f"\nResults Summary:")
    print(f"Total samples analyzed: {total_count}")
    print(f"Vulnerable code detected: {vulnerable_count} ({vulnerable_count/total_count*100:.2f}%)")
    print(f"Non-vulnerable code: {total_count - vulnerable_count} ({(total_count - vulnerable_count)/total_count*100:.2f}%)")

if __name__ == "__main__":
    main()

