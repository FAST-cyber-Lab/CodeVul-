{
 "cells": [
  {
   "cell_type": "raw",
   "id": "80b433b5-809f-4255-9f03-8ba4a099cafd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434faccb-517a-402f-b2e7-e2194906de1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/06zsyfqd5j3f3ynqdx1zsbbr0000gp/T/ipykernel_42555/132309276.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
      "/var/folders/gp/06zsyfqd5j3f3ynqdx1zsbbr0000gp/T/ipykernel_42555/132309276.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      4053\n",
      "           1       0.49      1.00      0.66      3947\n",
      "\n",
      "    accuracy                           0.49      8000\n",
      "   macro avg       0.25      0.50      0.33      8000\n",
      "weighted avg       0.24      0.49      0.33      8000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   0 4053]\n",
      " [   0 3947]]\n",
      "\n",
      "Accuracy: 0.4934\n",
      "Precision (Macro): 0.2467\n",
      "Recall (Macro): 0.5000\n",
      "F1 Score (Macro): 0.3304\n",
      "Precision (Weighted): 0.2434\n",
      "Recall (Weighted): 0.4934\n",
      "F1 Score (Weighted): 0.3260\n",
      "Matthews Correlation Coefficient: 0.0000\n",
      "Cohen's Kappa: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score, precision_score, \n",
    "    recall_score, f1_score, matthews_corrcoef, cohen_kappa_score\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "EMBED_DIM = 128\n",
    "MAX_PATHS = 100\n",
    "DROPOUT_RATE = 0.1\n",
    "LEARNING_RATE = 0.002\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "NUM_CLASSES = 5  # Updated for multi-class classification\n",
    "\n",
    "def ast_tokenize(code_snippet: str, max_seq_len: int = 50) -> torch.Tensor:\n",
    "    tokens = np.random.rand(max_seq_len, EMBED_DIM)\n",
    "    return torch.tensor(tokens, dtype=torch.float)\n",
    "\n",
    "class CodeSnippetDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.code_snippets = dataframe['functionSource'].tolist()\n",
    "        self.labels = dataframe['numeric'].tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.code_snippets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.code_snippets[idx], self.labels[idx]\n",
    "\n",
    "class ValueFlowPathEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(ValueFlowPathEncoder, self).__init__()\n",
    "        self.local_encoder = nn.LSTM(embed_dim, embed_dim, bidirectional=True, batch_first=True)\n",
    "        self.global_encoder = nn.LSTM(embed_dim * 2, embed_dim, batch_first=True)\n",
    "        self.attention = nn.Linear(embed_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        local_features, _ = self.local_encoder(x)\n",
    "        global_features, _ = self.global_encoder(local_features)\n",
    "        attn_weights = torch.softmax(self.attention(global_features), dim=1)\n",
    "        context = (attn_weights * global_features).sum(dim=1)\n",
    "        return context\n",
    "\n",
    "class TransformerBasedDetector(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, num_classes):\n",
    "        super(TransformerBasedDetector, self).__init__()\n",
    "        self.transformer = nn.Transformer(embed_dim, num_heads, num_layers, batch_first=True)\n",
    "        self.attention = nn.Linear(embed_dim, 1)\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)  # Updated for multi-class\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.transformer(x, x)\n",
    "        attn_weights = torch.softmax(self.attention(features), dim=1)\n",
    "        context = (attn_weights * features).sum(dim=1)\n",
    "        logits = self.classifier(context)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "def remo(code):\n",
    "    # Check if input is a string\n",
    "    if not isinstance(code, str):\n",
    "        return code\n",
    "        \n",
    "    code = re.sub(r'/\\.?\\*/', '', code, flags=re.DOTALL)\n",
    "    code = re.sub(r'//.*?$', '', code, flags=re.MULTILINE)\n",
    "    code = re.sub(r'^\\s*[\\n\\r]', '', code, flags=re.MULTILINE)\n",
    "    return code.strip()\n",
    "\n",
    "# Apply the function to the 'func' column (or whatever your code column is named)\n",
    "\n",
    "train = pd.read_csv(\"/Users/user01/fahim/icsme/train_label_dataset.csv\")\n",
    "test = pd.read_csv(\"/Users/user01/fahim/icsme/test_label_dataset.csv\")\n",
    "train['functionSource'] = train['functionSource'].apply(remo)\n",
    "test['functionSource'] = test['functionSource'].apply(remo)\n",
    "\n",
    "train = train[['functionSource', 'numeric']]\n",
    "test = test[['functionSource', 'numeric']]\n",
    "\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = CodeSnippetDataset(train)\n",
    "test_dataset = CodeSnippetDataset(test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = ValueFlowPathEncoder(EMBED_DIM).to(device)\n",
    "contrastive_loss = nn.CosineEmbeddingLoss()\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    encoder.train()\n",
    "    for code_snippets, _ in train_loader:\n",
    "        code_snippets = [ast_tokenize(snippet) for snippet in code_snippets]\n",
    "        code_snippets = torch.stack(code_snippets).to(device)\n",
    "        z_i, z_j = encoder(code_snippets), encoder(code_snippets)\n",
    "        target = torch.ones(z_i.size(0)).to(device)\n",
    "        loss = contrastive_loss(z_i, z_j, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "detector = TransformerBasedDetector(EMBED_DIM, num_heads=8, num_layers=2, num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(detector.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    detector.train()\n",
    "    for code_snippets, labels in train_loader:\n",
    "        code_snippets = [ast_tokenize(snippet) for snippet in code_snippets]\n",
    "        code_snippets = torch.stack(code_snippets).to(device)\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "        embeddings = encoder(code_snippets)\n",
    "        logits = detector(embeddings.unsqueeze(1))\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "detector.eval()\n",
    "encoder.eval()\n",
    "Y_pred, Y_true = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for code_snippets, labels in test_loader:\n",
    "        code_snippets = [ast_tokenize(snippet) for snippet in code_snippets]\n",
    "        code_snippets = torch.stack(code_snippets).to(device)\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "        embeddings = encoder(code_snippets)\n",
    "        logits = detector(embeddings.unsqueeze(1))\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        Y_pred.extend(predictions.cpu().numpy())\n",
    "        Y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_true, Y_pred, zero_division=0))\n",
    "\n",
    "conf_matrix = confusion_matrix(Y_true, Y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "accuracy = accuracy_score(Y_true, Y_pred)\n",
    "precision_macro = precision_score(Y_true, Y_pred, average=\"macro\", zero_division=0)\n",
    "recall_macro = recall_score(Y_true, Y_pred, average=\"macro\", zero_division=0)\n",
    "f1_macro = f1_score(Y_true, Y_pred, average=\"macro\", zero_division=0)\n",
    "precision_weighted = precision_score(Y_true, Y_pred, average=\"weighted\", zero_division=0)\n",
    "recall_weighted = recall_score(Y_true, Y_pred, average=\"weighted\", zero_division=0)\n",
    "f1_weighted = f1_score(Y_true, Y_pred, average=\"weighted\", zero_division=0)\n",
    "mcc = matthews_corrcoef(Y_true, Y_pred)\n",
    "kappa = cohen_kappa_score(Y_true, Y_pred)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision (Macro): {precision_macro:.4f}\")\n",
    "print(f\"Recall (Macro): {recall_macro:.4f}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "print(f\"Precision (Weighted): {precision_weighted:.4f}\")\n",
    "print(f\"Recall (Weighted): {recall_weighted:.4f}\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n",
    "print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f400b8-6a86-47d1-9840-b83154a45f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb67f324-01b6-460c-9974-0996a231ff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(Y_true, Y_pred)\n",
    "\n",
    "# Overall TP, FP, FN, TN\n",
    "TP = np.diag(conf_matrix).sum()  # Sum of diagonal elements (True Positives)\n",
    "FP = conf_matrix.sum(axis=0) - np.diag(conf_matrix)  # Column-wise sum minus TP\n",
    "FN = conf_matrix.sum(axis=1) - np.diag(conf_matrix)  # Row-wise sum minus TP\n",
    "TN = conf_matrix.sum() - (FP + FN + TP)  # Total elements minus others\n",
    "\n",
    "# Compute Sensitivity (Recall) & Specificity\n",
    "SN = TP / (TP + FN).sum()  # Sensitivity (Recall)\n",
    "SP = TN.sum() / (TN.sum() + FP.sum())  # Specificity\n",
    "\n",
    "# Compute MCC & Kappa\n",
    "mcc = matthews_corrcoef(Y_true, Y_pred)\n",
    "kappa = cohen_kappa_score(Y_true, Y_pred)\n",
    "\n",
    "# Compute MAE & MSE\n",
    "mae = mean_absolute_error(Y_true, Y_pred)\n",
    "mse = mean_squared_error(Y_true, Y_pred)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nOverall True Positives (TP): {TP}\")\n",
    "print(f\"Overall False Positives (FP): {FP.sum()}\")\n",
    "print(f\"Overall False Negatives (FN): {FN.sum()}\")\n",
    "print(f\"Overall True Negatives (TN): {TN.sum()}\")\n",
    "print(f\"\\nOverall Sensitivity (Recall): {SN:.4f}\")\n",
    "print(f\"Overall Specificity: {SP:.4f}\")\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3305a142-1407-46e8-9e4b-fa02d0546a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
