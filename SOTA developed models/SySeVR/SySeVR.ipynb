{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b90455c7-b7b6-4b49-881e-4b974bf51ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "GPU Available: 0\n",
      "Physical Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Physical Devices:\", tf.config.list_physical_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c9e5dc9-e3a7-44bd-9df4-845a76e32dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: gensim in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Collecting igraph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /packages/b0/17/621d3a59430851a327421fdbec9ec8494d7fadaffc6dfdd42d4a95accbf2/igraph-0.11.8-cp39-abi3-win_amd64.whl.metadata\n",
      "  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /packages/b0/17/621d3a59430851a327421fdbec9ec8494d7fadaffc6dfdd42d4a95accbf2/igraph-0.11.8-cp39-abi3-win_amd64.whl.metadata\n",
      "  WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /packages/b0/17/621d3a59430851a327421fdbec9ec8494d7fadaffc6dfdd42d4a95accbf2/igraph-0.11.8-cp39-abi3-win_amd64.whl.metadata\n",
      "  WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /packages/b0/17/621d3a59430851a327421fdbec9ec8494d7fadaffc6dfdd42d4a95accbf2/igraph-0.11.8-cp39-abi3-win_amd64.whl.metadata\n",
      "  WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /packages/b0/17/621d3a59430851a327421fdbec9ec8494d7fadaffc6dfdd42d4a95accbf2/igraph-0.11.8-cp39-abi3-win_amd64.whl.metadata\n",
      "ERROR: Could not install packages due to an OSError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Max retries exceeded with url: /packages/b0/17/621d3a59430851a327421fdbec9ec8494d7fadaffc6dfdd42d4a95accbf2/igraph-0.11.8-cp39-abi3-win_amd64.whl.metadata (Caused by ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting py2neo\n",
      "  Downloading py2neo-2021.2.4-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (from py2neo) (2025.1.31)\n",
      "Collecting interchange~=2021.0.4 (from py2neo)\n",
      "  Downloading interchange-2021.0.4-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting monotonic (from py2neo)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (from py2neo) (24.2)\n",
      "Collecting pansi>=2020.7.3 (from py2neo)\n",
      "  Downloading pansi-2024.11.0-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: pygments>=2.0.0 in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (from py2neo) (2.19.1)\n",
      "Requirement already satisfied: six>=1.15.0 in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (from py2neo) (1.17.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (from py2neo) (2.3.0)\n",
      "Requirement already satisfied: pytz in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (from interchange~=2021.0.4->py2neo) (2024.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\user01\\.conda\\envs\\myenv\\lib\\site-packages (from pansi>=2020.7.3->py2neo) (10.4.0)\n",
      "Downloading py2neo-2021.2.4-py2.py3-none-any.whl (177 kB)\n",
      "Downloading interchange-2021.0.4-py2.py3-none-any.whl (28 kB)\n",
      "Downloading pansi-2024.11.0-py2.py3-none-any.whl (26 kB)\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Installing collected packages: monotonic, pansi, interchange, py2neo\n",
      "Successfully installed interchange-2021.0.4 monotonic-1.6 pansi-2024.11.0 py2neo-2021.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas \n",
    "!pip install gensim scikit-learn\n",
    "!pip install igraph py2neo networkx\n",
    "!pip install py2neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaa605b1-0648-4173-a58c-842765070fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "def remo(code):\n",
    "    # Check if input is a string\n",
    "    if not isinstance(code, str):\n",
    "        return code\n",
    "        \n",
    "    code = re.sub(r'/\\.?\\*/', '', code, flags=re.DOTALL)\n",
    "    code = re.sub(r'//.*?$', '', code, flags=re.MULTILINE)\n",
    "    code = re.sub(r'^\\s*[\\n\\r]', '', code, flags=re.MULTILINE)\n",
    "    return code.strip()\n",
    "\n",
    "# Apply the function to the 'func' column (or whatever your code column is named)\n",
    "\n",
    "train = pd.read_csv(\"/Users/user01/fahim/icsme/train_label_dataset.csv\")\n",
    "test = pd.read_csv(\"/Users/user01/fahim/icsme/test_label_dataset.csv\")\n",
    "train['functionSource'] = train['functionSource'].apply(remo)\n",
    "test['functionSource'] = test['functionSource'].apply(remo)\n",
    "\n",
    "train = train[['functionSource', 'numeric']]\n",
    "test = test[['functionSource', 'numeric']]\n",
    "\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c4d798f-fdb9-406a-aa0f-bfc6cbab7f34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model saved.\n",
      "Training the BiGRU model...\n",
      "Epoch 1/5\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1141s\u001b[0m 8s/step - accuracy: 0.2781 - loss: 1.5826 - val_accuracy: 0.2498 - val_loss: 1.6222\n",
      "Epoch 2/5\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1362s\u001b[0m 10s/step - accuracy: 0.3268 - loss: 1.5392 - val_accuracy: 0.2522 - val_loss: 1.6271\n",
      "Epoch 3/5\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1546s\u001b[0m 11s/step - accuracy: 0.3377 - loss: 1.5180 - val_accuracy: 0.2669 - val_loss: 1.6164\n",
      "Epoch 4/5\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1657s\u001b[0m 12s/step - accuracy: 0.3436 - loss: 1.5123 - val_accuracy: 0.2433 - val_loss: 1.6292\n",
      "Epoch 5/5\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1728s\u001b[0m 12s/step - accuracy: 0.3423 - loss: 1.5103 - val_accuracy: 0.2633 - val_loss: 1.6223\n",
      "Evaluating the model...\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 1s/step - accuracy: 0.2624 - loss: 1.6194\n",
      "Test Loss: 1.6223, Test Accuracy: 0.2633\n",
      "Generating predictions...\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 924ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.71      0.35       934\n",
      "           1       0.32      0.21      0.25       860\n",
      "           2       0.28      0.18      0.22       918\n",
      "           3       0.48      0.11      0.17       909\n",
      "           4       0.32      0.09      0.14       879\n",
      "\n",
      "    accuracy                           0.26      4500\n",
      "   macro avg       0.33      0.26      0.23      4500\n",
      "weighted avg       0.33      0.26      0.23      4500\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[662 103  94  29  46]\n",
      " [534 180  88  16  42]\n",
      " [594  84 166  25  49]\n",
      " [567  87 122  97  36]\n",
      " [536 114 116  33  80]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Paths\n",
    "TRAIN_DATA_PATH = r'D:\\SySeVR\\train_dataset.csv'\n",
    "TEST_DATA_PATH = r'D:\\SySeVR\\test_dataset.csv'\n",
    "W2V_MODEL_PATH = r'D:\\SySeVR\\\\w2v_model\\wordmodel.model'\n",
    "EMBEDDING_DIM = 40\n",
    "MAXLEN = 198\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "OUTPUT_CLASSES = 5\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Load Dataset\n",
    "def load_data(train_path, test_path):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    return train_df, test_df\n",
    "\n",
    "# Step 2: Build PDG\n",
    "def build_pdg(source_code):\n",
    "    pdg = nx.DiGraph()\n",
    "    statements = source_code.split('\\n')\n",
    "    for i, stmt in enumerate(statements):\n",
    "        pdg.add_node(i, code=stmt, type=\"Statement\", location=f\"{i+1}:0\")\n",
    "    for i in range(len(statements) - 1):\n",
    "        pdg.add_edge(i, i + 1, type=\"ControlDependency\")\n",
    "    for i, stmt in enumerate(statements):\n",
    "        if \"=\" in stmt:\n",
    "            var_name = stmt.split(\"=\")[0].strip()\n",
    "            for j, other_stmt in enumerate(statements):\n",
    "                if var_name in other_stmt and i != j:\n",
    "                    pdg.add_edge(i, j, type=\"DataDependency\", var=var_name)\n",
    "    return pdg\n",
    "\n",
    "# Step 3: Modify PDG Nodes\n",
    "def modify_pdg_nodes(pdg):\n",
    "    for node, data in pdg.nodes(data=True):\n",
    "        if data['type'] == \"Statement\":\n",
    "            data['code'] = data['code'].strip()\n",
    "    return pdg\n",
    "\n",
    "# Step 4: Extract Slices from PDG\n",
    "def extract_slices_from_pdg(pdg):\n",
    "    slices = []\n",
    "    for node in pdg.nodes:\n",
    "        slice_nodes = list(nx.ancestors(pdg, node)) + [node]\n",
    "        slices.append([pdg.nodes[n]['code'] for n in slice_nodes])\n",
    "    return slices\n",
    "\n",
    "# Step 5: Process Functions\n",
    "\n",
    "def process_functions_with_pdg_grouped(df):\n",
    "    grouped_slices = []\n",
    "    for func in df['functionSource']:\n",
    "        pdg = build_pdg(func)\n",
    "        pdg = modify_pdg_nodes(pdg)\n",
    "        slices = extract_slices_from_pdg(pdg)\n",
    "        grouped_slices.append(slices)\n",
    "    return grouped_slices\n",
    "\n",
    "# Step 6: Train Word2Vec Model\n",
    "def train_word2vec(corpus, model_path):\n",
    "    \n",
    "    w2v_model = Word2Vec(sentences=corpus, vector_size=EMBEDDING_DIM, window=5, min_count=1, workers=4, sg=1, epochs=5)\n",
    "    w2v_model.save(model_path)\n",
    "    print(\"Word2Vec model saved.\")\n",
    "\n",
    "# Step 7: Pad and Embed Functions\n",
    "def pad_grouped_slices(grouped_vectors, maxlen, embedding_dim, max_slices):\n",
    "    padded_grouped = []\n",
    "    for group in grouped_vectors:\n",
    "        padded_group = []\n",
    "        for vec in group[:max_slices]:\n",
    "            if len(vec) > maxlen:\n",
    "                vec = vec[:maxlen]\n",
    "            elif len(vec) < maxlen:\n",
    "                vec.extend([[0] * embedding_dim] * (maxlen - len(vec)))\n",
    "            padded_group.append(vec)\n",
    "        while len(padded_group) < max_slices:\n",
    "            padded_group.append([[0] * embedding_dim] * maxlen)\n",
    "        padded_grouped.append(padded_group)\n",
    "    return np.array(padded_grouped, dtype=np.float32)\n",
    "\n",
    "def embed_grouped_tokens(grouped_tokens, w2v_model):\n",
    "    embedded_grouped = []\n",
    "    for token_group in grouped_tokens:\n",
    "        embedded_group = [\n",
    "            [w2v_model.wv[token] for token in token_list if token in w2v_model.wv]\n",
    "            for token_list in token_group\n",
    "        ]\n",
    "        embedded_group = [emb_slice for emb_slice in embedded_group if emb_slice]  # Remove empty slices\n",
    "        if not embedded_group:  # Add placeholder for empty groups\n",
    "            embedded_group = [[[0] * w2v_model.vector_size] * MAXLEN]\n",
    "        embedded_grouped.append(embedded_group)\n",
    "    return embedded_grouped\n",
    "    \n",
    "# Step 8: Define TensorFlow BiGRU Model\n",
    "def create_bgru_model(input_dim, maxlen, embedding_dim, hidden_dim, output_dim, dropout):\n",
    "    inputs = Input(shape=(maxlen, embedding_dim))\n",
    "    x = Bidirectional(GRU(hidden_dim, return_sequences=False, dropout=dropout))(inputs)\n",
    "    x = Dense(output_dim, activation=\"softmax\")(x)\n",
    "    model = Model(inputs, x)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # train_df, test_df = load_data(TRAIN_DATA_PATH, TEST_DATA_PATH)\n",
    "\n",
    "    \n",
    "    # label_encoder = LabelEncoder()\n",
    "    # train_df['label'] = label_encoder.fit_transform(train_df['label'])\n",
    "    # test_df['label'] = label_encoder.transform(test_df['label'])\n",
    "\n",
    "    # Process functions and construct PDGs\n",
    "    \n",
    "    train_grouped_slices = process_functions_with_pdg_grouped(train)\n",
    "    \n",
    "    test_grouped_slices = process_functions_with_pdg_grouped(test)\n",
    "\n",
    "    # Train Word2Vec embeddings\n",
    "    \n",
    "    train_word2vec([slice_ for group in train_grouped_slices for slice_ in group], W2V_MODEL_PATH)\n",
    "\n",
    "    # Load Word2Vec model\n",
    "    w2v_model = Word2Vec.load(W2V_MODEL_PATH)\n",
    "\n",
    "    # Embed grouped tokens\n",
    "    \n",
    "    train_grouped_vectors = embed_grouped_tokens(train_grouped_slices, w2v_model)\n",
    "    \n",
    "    test_grouped_vectors = embed_grouped_tokens(test_grouped_slices, w2v_model)\n",
    "\n",
    " \n",
    "    MAX_SLICES = 9 \n",
    "    train_padded = pad_grouped_slices(train_grouped_vectors, MAXLEN, EMBEDDING_DIM, MAX_SLICES)\n",
    "   \n",
    "    test_padded = pad_grouped_slices(test_grouped_vectors, MAXLEN, EMBEDDING_DIM, MAX_SLICES)\n",
    "\n",
    "\n",
    "    # Prepare labels\n",
    "    train_labels = train['numeric'].values\n",
    "    test_labels = test['numeric'].values\n",
    "\n",
    "    \n",
    "    train_labels = to_categorical(train_labels, num_classes=OUTPUT_CLASSES)\n",
    "    test_labels = to_categorical(test_labels, num_classes=OUTPUT_CLASSES)\n",
    "    train_padded = train_padded.reshape(train_padded.shape[0], -1, EMBEDDING_DIM)\n",
    "    test_padded = test_padded.reshape(test_padded.shape[0], -1, EMBEDDING_DIM)\n",
    "\n",
    "    \n",
    "    assert train_padded.shape[0] == len(train_labels), \"Mismatch between train data and labels\"\n",
    "    assert test_padded.shape[0] == len(test_labels), \"Mismatch between test data and labels\"\n",
    "\n",
    "   \n",
    "    model = create_bgru_model(\n",
    "        input_dim=EMBEDDING_DIM,\n",
    "        maxlen=train_padded.shape[1],\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=256,\n",
    "        output_dim=OUTPUT_CLASSES,\n",
    "        dropout=0.2,\n",
    "    )\n",
    "\n",
    "   \n",
    "    print(\"Training the BiGRU model...\")\n",
    "    model.fit(\n",
    "        train_padded,\n",
    "        train_labels,\n",
    "        validation_data=(test_padded, test_labels),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating the model...\")\n",
    "    test_loss, test_accuracy = model.evaluate(test_padded, test_labels, verbose=1)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Get predictions\n",
    "    print(\"Generating predictions...\")\n",
    "    y_pred_probs = model.predict(test_padded)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.argmax(test_labels, axis=1)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0112d911-92c1-4873-8817-1b0b2a830b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2633333333333333\n",
      "Precision (Macro): 0.3260423197148011\n",
      "Recall (Macro): 0.259326568166853\n",
      "F1 Score (Macro): 0.22701670881899833\n",
      "Precision (Weighted): 0.32558182250816703\n",
      "Recall (Weighted): 0.2633333333333333\n",
      "F1 Score (Weighted): 0.2279629982729952\n",
      "Matthews Correlation Coefficient (MCC): 0.08944184122618962\n",
      "Cohen's Kappa: 0.07444306973310644\n",
      "Mean Squared Error (MSE): 4.650888888888889\n",
      "Mean Absolute Error (MAE): 1.6713333333333333\n",
      "ROC AUC Score (Macro): 0.5619550782717949\n",
      "True Positives (TP): 1185\n",
      "True Negatives (TN): 1425\n",
      "False Positives (FP): 3315\n",
      "False Negatives (FN): 3315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    matthews_corrcoef, cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_true and y_pred are already defined as in your code\n",
    "\n",
    "# Calculate overall metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "kappa = cohen_kappa_score(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# For ROC AUC Score, we need the probability estimates for each class\n",
    "# Assuming y_pred_probs contains these probabilities\n",
    "try:\n",
    "    roc_auc_macro = roc_auc_score(y_true, y_pred_probs, average='macro', multi_class='ovr')\n",
    "except ValueError:\n",
    "    roc_auc_macro = float('nan')  # Handle case where ROC AUC cannot be computed\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn = np.sum(np.diag(cm)) - np.sum(cm, axis=1)  # True Negatives\n",
    "fp = np.sum(cm, axis=0) - np.diag(cm)          # False Positives\n",
    "fn = np.sum(cm, axis=1) - np.diag(cm)          # False Negatives\n",
    "tp = np.diag(cm)                               # True Positives\n",
    "\n",
    "# Summing up for overall counts\n",
    "TP = np.sum(tp)\n",
    "TN = np.sum(tn)\n",
    "FP = np.sum(fp)\n",
    "FN = np.sum(fn)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision (Macro): {precision_macro}\")\n",
    "print(f\"Recall (Macro): {recall_macro}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro}\")\n",
    "print(f\"Precision (Weighted): {precision_weighted}\")\n",
    "print(f\"Recall (Weighted): {recall_weighted}\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted}\")\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc}\")\n",
    "print(f\"Cohen's Kappa: {kappa}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"ROC AUC Score (Macro): {roc_auc_macro}\")\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3bb0388-8ba7-42d2-922e-dde870c1eb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity (Macro-Averaged): 0.2593\n",
      "Specificity (Macro-Averaged): 0.8148\n"
     ]
    }
   ],
   "source": [
    "sensitivity_list = []\n",
    "specificity_list = []\n",
    "\n",
    "# Calculate Sensitivity and Specificity for each class\n",
    "for i in range(len(cm)):\n",
    "    TP = cm[i, i]\n",
    "    FN = np.sum(cm[i, :]) - TP\n",
    "    FP = np.sum(cm[:, i]) - TP\n",
    "    TN = np.sum(cm) - (TP + FP + FN)\n",
    "    \n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    \n",
    "    sensitivity_list.append(sensitivity)\n",
    "    specificity_list.append(specificity)\n",
    "\n",
    "# Calculate macro-averaged Sensitivity and Specificity\n",
    "sensitivity_macro = np.mean(sensitivity_list)\n",
    "specificity_macro = np.mean(specificity_list)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Sensitivity (Macro-Averaged): {sensitivity_macro:.4f}\")\n",
    "print(f\"Specificity (Macro-Averaged): {specificity_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342da48f-6e50-41e3-991c-e1b3d0f57504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-myenv]",
   "language": "python",
   "name": "conda-env-.conda-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
